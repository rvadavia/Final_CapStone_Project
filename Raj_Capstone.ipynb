{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract: This notebook is used for the Final capstone project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "import base64\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "from skimage import feature\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlity functios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileWriter:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.file = open(self.filename, 'a')\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.file.close()\n",
    "\n",
    "    def append_to_file(self, text):\n",
    "        self.file.write(text + '\\n')\n",
    "\n",
    "    # def Creat_file(self,text)\n",
    "    #      with open(output_file, \"w\") as outfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Write_Readme_file():\n",
    "    \n",
    "    # names of the files to be concatenated\n",
    "    file1 = \"README_Org.md\"\n",
    "    file2 = \"output.md\"\n",
    "    file3 = \"Recomdation.md\"\n",
    "\n",
    "    # name of the output file\n",
    "    output_file = \"README.md\"\n",
    "\n",
    "    # open the output file in write mode\n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        # open the first file in read mode\n",
    "        with open(file1, \"r\") as infile:\n",
    "            # write its contents to the output file\n",
    "            content = infile.read()\n",
    "            # print(f\"Content of {file1}:\")\n",
    "            # print(content)\n",
    "            outfile.write(content)\n",
    "\n",
    "        # write a newline to separate the contents of the two files\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "        # open the second file in read mode\n",
    "        with open(file2, \"r\") as infile:\n",
    "            # write its contents to the output file\n",
    "            content = infile.read()\n",
    "            # print(f\"Content of {file2}:\")\n",
    "            # print(content)\n",
    "            outfile.write(content)\n",
    "\n",
    "        # open the thred file in read mode\n",
    "        with open(file3, \"r\") as infile:\n",
    "            # write its contents to the output file\n",
    "            content = infile.read()\n",
    "            # print(f\"Content of {file2}:\")\n",
    "            # print(content)\n",
    "            outfile.write(content)\n",
    "    outfile.close()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A balanced dataset for image categorization means that there are roughly equal numbers of images for each class or category in the dataset. You can check the balance of your dataset by counting the number of instances of each class\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the dataset and balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlanceDataste:\n",
    "    def __init__(self, df, output_csv, featuresList):\n",
    "        self.df = df\n",
    "        self.output_csv = output_csv\n",
    "        self.featuresList = featuresList\n",
    "        \n",
    "    def process_Data(self):\n",
    "        try:\n",
    "            # Separate features and target variables\n",
    "             features = self.df.drop(self.featuresList, axis=1)  # Replace with your actual column names\n",
    "             targets = self.df[self.featuresList]  # Replace with your actual column names\n",
    "             \n",
    "        # Perform balancing for each target variable separately\n",
    "             balanced_features = []\n",
    "             balanced_targets = []\n",
    "\n",
    "             for column in targets.columns:\n",
    "                target = targets[column]\n",
    "                \n",
    "                # Perform undersampling\n",
    "                undersampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "                features_undersampled, target_undersampled = undersampler.fit_resample(features, target)\n",
    "                \n",
    "                # Perform oversampling on the undersampled data\n",
    "                oversampler = RandomOverSampler(sampling_strategy='minority')\n",
    "                features_balanced, target_balanced = oversampler.fit_resample(features_undersampled, target_undersampled)\n",
    "                \n",
    "                balanced_features.append(features_balanced)\n",
    "                balanced_targets.append(target_balanced)\n",
    "\n",
    "                # Create a new DataFrame with balanced data\n",
    "                balanced_df = pd.concat([pd.DataFrame(feature) for feature in balanced_features] + [pd.DataFrame(target) for target in balanced_targets], axis=1)\n",
    "\n",
    "                # Write the balanced data to a new CSV file\n",
    "                balanced_df.to_csv(self.output_csv, index=False)\n",
    "                \n",
    "        except IOError:\n",
    "                    print(f\"Error creating image {self.output_csv}\")\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.scaler = StandardScaler()\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.outfutfile = \"output.md\"\n",
    "     \n",
    "    def report_and_recommend(self):\n",
    "        \n",
    "        title = (\"\\n Recommendations:\")\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "            writer.append_to_file(title)\n",
    "            for column in self.df.columns:\n",
    "                if self.df[column].isnull().sum() > 0:\n",
    "                        writer.append_to_file(f\" \\n  Column '{column}' has missing values. Consider using dropna() or fillna().\")\n",
    "                            \n",
    "\n",
    "                if self.df[column].dtype == 'object':\n",
    "                        writer.append_to_file(f\" \\n  Column '{column}' is categorical. Consider encoding it using label encoding or one-hot encoding.\")\n",
    "\n",
    "\n",
    "                elif self.df[column].dtype in ['int64', 'float64']:\n",
    "                        writer.append_to_file(f\" \\n  Column '{column}' is numerical. Consider scaling it using standard scaling or min-max scaling.\")\n",
    "        \n",
    "            writer.append_to_file(\"\\n Recommendations - END -:\")        \n",
    "       \n",
    "   \n",
    "    def drop_missing(self):\n",
    "        self.df.dropna(inplace=True)\n",
    "\n",
    "    def fill_missing(self, column, value):\n",
    "        self.df[column].fillna(value, inplace=True)\n",
    "\n",
    "    def encode_categorical(self, column):\n",
    "        self.df[column] = self.encoder.fit_transform(self.df[column])\n",
    "\n",
    "    def scale_numerical(self, column):\n",
    "        self.df[column] = self.scaler.fit_transform(self.df[column].values.reshape(-1, 1))\n",
    "\n",
    "    def fill_missing_numeric_With_mean(self, column1, column2):\n",
    "        # Calculate the mean of 'column1' for each category in 'column2'\n",
    "        mean_values = self.df.groupby(column2)[column1].mean()\n",
    "\n",
    "        # Fill the missing values in 'column1' with the mean of 'column1' in the same category in 'column2'\n",
    "        self.df[column1] = self.df.apply(\n",
    "             lambda row: mean_values[row[column2]] if pd.isnull(row[column1]) else row[column1],axis=1)\n",
    "        \n",
    "    def fill_missing_Category_values(self, column1, column2):\n",
    "       \n",
    "        # # Define a lambda function to compute the mode\n",
    "        fill_mode = lambda x: x.fillna(x.mode().iloc[0])\n",
    "\n",
    "        # # Fill missing values in column1 column with the mode of the corresponding group in column2\n",
    "        self.df[column1] = self.df.groupby(column2)[column1].transform(fill_mode)\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlanceImages:\n",
    "    def __init__(self, Image_fiel_path, df, FinalImage_dir, output_csv,  size=(128, 128) ):\n",
    "        self.df = df\n",
    "        self.FinalImage_dir = FinalImage_dir\n",
    "        self.output_csv = output_csv\n",
    "        self.Image_fiel_path = Image_fiel_path\n",
    "        self.size = size \n",
    "\n",
    "    def process_image(self, file_path, newfile):\n",
    "\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            img = img.resize(self.size)  # Resize image\n",
    "            # Convert image to numpy array\n",
    "            img_array = np.array(img)\n",
    "\n",
    "            # Normalize to [0,1]\n",
    "            img_normalized = img_array / 255.\n",
    "           # Convert back to image\n",
    "            img = Image.fromarray((img_normalized * 255).astype(np.uint8))\n",
    "            img.save(newfile)\n",
    "            \n",
    "        except IOError:\n",
    "            print(f\"Error processing image {file_path}\")\n",
    "            return False\n",
    "        return True \n",
    "\n",
    "    def process_images(self): \n",
    "        \n",
    "        for index, row in self.df.iterrows():\n",
    "            file_name = row['image_name'] +'.jpg'\n",
    "            row_image_file = os.path.join(self.Image_fiel_path, file_name)\n",
    "            newfile =  os.path.join(self.FinalImage_dir, file_name)\n",
    "\n",
    "            self.process_image(row_image_file, newfile) \n",
    "\n",
    "                              \n",
    "            pass\n",
    "        \n",
    "        self.df.to_csv(self.output_csv,index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class removeCorruptImage:\n",
    "    def __init__(self, Image_fiel_path, df, corrupt_dir, output_csv ):\n",
    "        self.df = df\n",
    "        self.corrupt_dir = corrupt_dir\n",
    "        self.output_csv = output_csv\n",
    "        self.Image_fiel_path = Image_fiel_path\n",
    "        self.outfutfile = \"output.md\"\n",
    "        self.nonCorruptoutput_csv = 'nonCorruptoutput'\n",
    "        \n",
    "    def is_corrupt(self, file_path):\n",
    "        try:\n",
    "            Image.open(file_path)\n",
    "        except IOError:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def process_images(self): \n",
    "        \n",
    "        count = 0\n",
    "        ccount = 0\n",
    "        for index, row in self.df.iterrows():\n",
    "            count = count + 1\n",
    "            file_name = row['image_name'] +'.jpg'\n",
    "            row_image_file = os.path.join(self.Image_fiel_path, file_name)\n",
    "            if self.is_corrupt(row_image_file):\n",
    "                cccount = cccount +1\n",
    "                shutil.move(row_image_file, self.corrupt_dir)  # Move corrupt files to a separate directory\n",
    "                self.df.drop(index, inplace=True)  # Remove the row from the DataFrame\n",
    "               \n",
    "                        \n",
    "            pass\n",
    "        \n",
    "        self.df.to_csv(self.nonCorruptoutput_csv,index=False)\n",
    "\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "                    writer.append_to_file(\"\\n ------------  -----------------  --------------   --------------\\n\")\n",
    "                    Message1 = f\" ### proces directery to see if there is any currept imges\"\n",
    "                    writer.append_to_file(Message1)\n",
    "                    Message = f\" \\n Total Image file count is {count} \\n Total currept imge files count is {ccount}\\n\"\n",
    "                    writer.append_to_file(Message)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_Comparison:\n",
    "    def __init__(self, df1, df2):\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "        self.outfutfile = \"output.md\"\n",
    "        headers = ['BeforImg', 'AfterImg']\n",
    "        # Create an empty DataFrame with headers\n",
    "        self.ImageDF = pd.DataFrame(columns=headers, index=[0])\n",
    "\n",
    "       \n",
    "    def report(self):\n",
    "        \n",
    "        self.print_info(self.df1,\"\\n ## DataFrame info befor process :\")\n",
    "        self.print_Missing_values(self.df1,\"\\n ## Missing values befor & aftre  process :\")\n",
    "             \n",
    "        # print(\"\\nUnique values df1 - df2:\")\n",
    "        title = \"\\n ## Unique values befor & aftre  process :\"\n",
    "        self.print_unique_values(title)\n",
    "\n",
    "        self.print_value_count(\"\\n ## Value counts befor & aftre  process :\")\n",
    "        title = \"\\n ## Descriptive statistics befor and after the process:\"\n",
    "\n",
    "\n",
    "\n",
    "        # Getting the description of both DataFrames\n",
    "        desc_df1 = self.df1.describe()\n",
    "        desc_df2 = self.df2.describe()\n",
    "\n",
    "        # Combine the two descriptions into one DataFrame\n",
    "        combined_desc = pd.concat([desc_df1, desc_df2], axis=1)\n",
    "\n",
    "        markdown_str = combined_desc.to_markdown()\n",
    "\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "            writer.append_to_file(title)\n",
    "            writer.append_to_file(markdown_str)\n",
    "            writer.append_to_file('\\n')\n",
    "\n",
    "      \n",
    "\n",
    "        \n",
    "    def print_info(self,df,title):\n",
    "\n",
    "        \n",
    "        # Capture df.info() output in a string\n",
    "        buf = io.StringIO()\n",
    "        df.info(buf=buf)\n",
    "        info_str = buf.getvalue()\n",
    "\n",
    "        # Create a summary DataFrame\n",
    "        info_list = info_str.split('\\n')[5:-3]  # Remove first and last two lines\n",
    "        info_data = [line.split() for line in info_list]  # Split each line into list of words\n",
    "        # Create dataframe from info_data\n",
    "        info_df = pd.DataFrame(info_data, columns=['index', 'Name', 'Count', 'Non-Null','Dtype'])\n",
    "\n",
    "        # Print the summary DataFrame in markdown format\n",
    "        # print(info_df.to_markdown(index=False))\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        markdown_str = info_df.to_markdown()\n",
    "\n",
    "        # Write the markdown string to a text file\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "            writer.append_to_file(title)\n",
    "            writer.append_to_file(markdown_str)\n",
    "            writer.append_to_file('\\n')\n",
    "    \n",
    "    \n",
    "    def print_Missing_values(self,df,title):\n",
    "\n",
    "        # Get the count of null values in each column\n",
    "        null_counts1 = self.df1.isnull().sum()\n",
    "        null_counts2 = self.df2.isnull().sum()\n",
    "        null_counts = pd.DataFrame({'Befor':null_counts1,'After':null_counts2})\n",
    "        \n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "             writer.append_to_file(title)\n",
    "             writer.append_to_file(null_counts.to_markdown())\n",
    "             writer.append_to_file('\\n')\n",
    "\n",
    "    def print_unique_values(self,title):\n",
    "\n",
    "        unique_counts1 = self.df1.nunique()\n",
    "        unique_counts2 = self.df2.nunique()\n",
    "        unique_counts = pd.DataFrame({'Befor':unique_counts1,'After':unique_counts2})\n",
    "\n",
    "        #print(unique_counts)    \n",
    "        # Write the markdown string to a text file\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "             writer.append_to_file(title)\n",
    "             writer.append_to_file(unique_counts.to_markdown())\n",
    "             writer.append_to_file('\\n')\n",
    "\n",
    "    def print_value_count(self,title):\n",
    "\n",
    "        # Create an empty DataFrame to store the results\n",
    "        result = pd.DataFrame()\n",
    "        \n",
    "        # Iterate over each column\n",
    "        for column in self.df1.columns:\n",
    "\n",
    "            if self.df1[column].nunique() > 8:\n",
    "                continue\n",
    "\n",
    "            df1_counts = self.df1[column].value_counts()\n",
    "            df2_counts = self.df2[column].value_counts()\n",
    "\n",
    "            # Combine the two Series into a DataFrame\n",
    "            temp_df = pd.concat([df1_counts, df2_counts], axis=1, keys=['DF1', 'DF2'])\n",
    "\n",
    "            # Add the column name to the DataFrame\n",
    "            temp_df['Column'] = column\n",
    "\n",
    "            # Append the temporary DataFrame to the result DataFrame\n",
    "            \n",
    "            result = pd.concat([result, temp_df])\n",
    "            \n",
    "\n",
    "        # Reset the index of the result DataFrame\n",
    "        result.reset_index(inplace=True)\n",
    "\n",
    "        # Rename the columns for clarity\n",
    "        result.columns = ['Value', 'Count Befor', 'Count After', 'Column']\n",
    "\n",
    "        # Rearrange the columns\n",
    "        result = result[['Column', 'Value', 'Count Befor', 'Count After']]\n",
    "\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "             writer.append_to_file(title)\n",
    "             writer.append_to_file(result.to_markdown())\n",
    "             writer.append_to_file('\\n')\n",
    "\n",
    "    def visualize(self, column):\n",
    "            \n",
    "            label1 = 'BfeorImg'\n",
    "            label2 = 'AfterImg'\n",
    "    \n",
    "            if self.df1[column].dtype in ['int64', 'float64']:\n",
    "\n",
    "                fname1 = f\"media//Histogram_of_{label1}_{column}.png\" \n",
    "                \n",
    "                ax = sns.histplot(self.df1[column])\n",
    "                ax.set_title(f\"Histogram of _{label1}_{column}\")\n",
    "                \n",
    "\n",
    "                plt.xticks(rotation=90) \n",
    "                plt.xlabel(column)\n",
    "\n",
    "                # Adjust the plot limits and aspect ratio\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                plt.savefig(fname1)\n",
    "\n",
    "                fname2 = f\"media//Histogram_of_{label2}_{column}.png\" \n",
    "                ax = sns.histplot(self.df2[column])\n",
    "                ax.set_title(f\"Histogram of _{label2}_{column}\")\n",
    "\n",
    "                plt.xticks(rotation=90) \n",
    "                plt.xlabel(column)\n",
    "                # Adjust the plot limits and aspect ratio\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(fname2)\n",
    "              \n",
    "\n",
    "                # Create a new DataFrame with the entry to be added\n",
    "                new_entry = pd.DataFrame({'BeforImg': fname1, 'AfterImg': fname2},index=[0])\n",
    "\n",
    "                # Concatenate the new DataFrame with the existing DataFrame\n",
    "                self.ImageDF = pd.concat([self.ImageDF, new_entry], ignore_index=True)\n",
    "\n",
    "             \n",
    "                \n",
    "            else:\n",
    "\n",
    "                fname1 = f\"media//Count_plot_of_{label1}_{column}.png\"\n",
    "                ax = sns.countplot(x=column, data=self.df1)\n",
    "                ax.set_title(f\"Count plot of _{label1}_{column}\")\n",
    "                plt.xticks(rotation=90) \n",
    "                # Adjust the plot limits and aspect ratio\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(fname1)\n",
    "\n",
    "                fname2 = f\"media//Count_plot_of_{label2}_{column}.png\"\n",
    "                ax = sns.countplot(x=column, data=self.df2)\n",
    "                ax.set_title(f\"Count plot of _{label2}_{column}\")\n",
    "                \n",
    "                # Adjust the plot limits and aspect ratio\n",
    "                \n",
    "                plt.xticks(rotation=90)\n",
    "                plt.tight_layout() \n",
    "                plt.savefig(fname2)\n",
    "                \n",
    "\n",
    "                # Create a new DataFrame with the entry to be added\n",
    "                new_entry = pd.DataFrame({'BeforImg': fname1, 'AfterImg': fname2}, index=[0])\n",
    "\n",
    "                # Concatenate the new DataFrame with the existing DataFrame\n",
    "                self.ImageDF = pd.concat([self.ImageDF, new_entry], ignore_index=True)\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "    def writeImages(self):\n",
    "\n",
    "        self.ImageDF = self.ImageDF.drop(self.ImageDF.index[0])\n",
    "        markup_code =  '\\n'\n",
    "        markup_code += '|:-------------------------------------:|:-----------------------------------:|\\n'\n",
    "        markup_code += '|:---------  Befor --------------------:|:------------ Aftre ----------------:|\\n'\n",
    "        markup_code += '\\n'\n",
    "        markup_code +=  f'| ![Befor]({self.ImageDF.iloc[0,0]}) | ![After]({self.ImageDF.iloc[0,1]})|\\n'\n",
    "        markup_code += '|:------------------------------------:|:--------------------------------------:|\\n'\n",
    "        markup_code +=  f'| ![Befor]({self.ImageDF.iloc[1,0]}) | ![After]({self.ImageDF.iloc[1,1]})|\\n'\n",
    "        markup_code += '|:------------------------------------:|:--------------------------------------:|\\n'\n",
    "        markup_code +=  f'| ![Befor]({self.ImageDF.iloc[2,0]}) | ![After]({self.ImageDF.iloc[2,1]})|\\n'\n",
    "        markup_code += '|:------------------------------------:|:--------------------------------------:|\\n'\n",
    "        markup_code +=  f'| ![Befor]({self.ImageDF.iloc[3,0]}) | ![After]({self.ImageDF.iloc[3,1]})|\\n'\n",
    "        markup_code += '|:------------------------------------:|:--------------------------------------:|\\n'        \n",
    "        markup_code +=  f'| ![Befor]({self.ImageDF.iloc[4,0]}) | ![After]({self.ImageDF.iloc[4,1]})|\\n'\n",
    "        markup_code += '|:------------------------------------:|:--------------------------------------:|\\n'\n",
    "        markup_code +=  f'| ![Befor]({self.ImageDF.iloc[5,0]}) | ![After]({self.ImageDF.iloc[5,1]})|\\n'  \n",
    "        markup_code += '|:------------------------------------:|:--------------------------------------:|\\n'  \n",
    "\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "            writer.append_to_file(\"Bfeor and after \")\n",
    "            writer.append_to_file(markup_code)\n",
    "            writer.append_to_file('\\n')\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DF_apply_LabelEncoder:\n",
    "    def __init__(self, df, colName):\n",
    "        self.df = df\n",
    "        self.colName = colName\n",
    "        self.outfutfile = \"output.md\"\n",
    "\n",
    "    def encode(self):\n",
    "    # Label Encoding\n",
    "        label_encoder = LabelEncoder()\n",
    "        Label = self.colName + \"_\" + \"encoded\"\n",
    "        self.df[Label] = label_encoder.fit_transform(self.df[self.colName])\n",
    "    # Return the transformed DataFrame\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_apply_StandardScaler:\n",
    "    def __init__(self, df, colName):\n",
    "        self.df = df\n",
    "        self.colName = colName\n",
    "        self.outfutfile = \"output.md\"\n",
    "\n",
    "    def encode(self):\n",
    "    # Label Encoding\n",
    "        scaler = StandardScaler()\n",
    "        Label = self.colName + \"_\" + \"encoded\"\n",
    "        self.df[Label] = scaler.fit_transform(self.df[self.colName])\n",
    "    # Return the transformed DataFrame\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define row CSV file name and path\n",
    "\n",
    "# Current directory\n",
    "current_dir = os.getcwd()\n",
    "# Parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "Data_path_Name= 'melanoma-classification_data/'\n",
    "Train_File_name_pre = 'train_pre.csv'\n",
    "Train_Uncurept_Image_File_Name = 'train_UImage.csv'\n",
    "Test_File_name_pre = 'test_pre.csv'\n",
    "\n",
    "Image_File_Path_pre_train = os.path.join(parent_dir, '\\\\Train')\n",
    "Image_File_Path_post_train = os.path.join(current_dir, 'melanoma-classification_data\\jpeg\\Train')\n",
    "\n",
    "Org_csv_file = os.path.join(Data_path_Name, Train_File_name_pre)\n",
    "Post_scv_file = os.path.join(Data_path_Name, Train_Uncurept_Image_File_Name)\n",
    "md_file = os.path.join(current_dir, 'output.md')\n",
    "\n",
    "UCorrupt_csv_file_name = 'nonCorruptoutput.csv'\n",
    "UCorrupt_csv_file = os.path.join(Data_path_Name,UCorrupt_csv_file_name)\n",
    "\n",
    "try:\n",
    "    os.remove(md_file)\n",
    "except IOError:\n",
    "    pass\n",
    "            \n",
    "\n",
    "Blanced_csv_file_name = 'Blanced.csv'\n",
    "Blanced_csv_file = os.path.join(Data_path_Name, Blanced_csv_file_name)\n",
    "\n",
    "df_org  = pd.read_csv(Org_csv_file)\n",
    "\n",
    "# # rename long column name anatom_site_general_challenge to short name anatomy_sites\n",
    "df_org.rename(columns={\"anatom_site_general_challenge\": \"anatomy_sites\"}, inplace=True) # rename column name\n",
    "\n",
    "# # # #  if there any currupt Image files Remove them and cetar new csv file\n",
    "processor = removeCorruptImage(Image_File_Path_pre_train , df_org , 'corrupt_files', UCorrupt_csv_file)\n",
    "processor.process_images()\n",
    "\n",
    "df_UImage = pd.read_csv(UCorrupt_csv_file)\n",
    "cleanData = DataCleaner(df_UImage)\n",
    "cleanData.report_and_recommend()\n",
    "# fill in miising values\n",
    "cleanData.fill_missing_numeric_With_mean('age_approx', 'diagnosis')\n",
    "cleanData.fill_missing_Category_values('anatomy_sites', 'diagnosis')\n",
    "cleanData.fill_missing_Category_values('sex', 'diagnosis')\n",
    "\n",
    "df_UImage = cleanData.df\n",
    "\n",
    "# # Create a blanced dataset arount benign_malignant feature\n",
    "\n",
    "featuresList = ['benign_malignant']\n",
    "Blancer = BlanceDataste(df_UImage,Blanced_csv_file,featuresList)\n",
    "Blancer.process_Data()\n",
    "\n",
    "df_Blanced  = pd.read_csv(Blanced_csv_file)\n",
    "\n",
    "DF_ComparisonX = DF_Comparison(df_UImage, df_Blanced)\n",
    "DF_ComparisonX.report()\n",
    "DF_ComparisonX.visualize('sex')\n",
    "DF_ComparisonX.visualize('age_approx')\n",
    "DF_ComparisonX.visualize('anatomy_sites')\n",
    "DF_ComparisonX.visualize('target')\n",
    "DF_ComparisonX.visualize('benign_malignant')\n",
    "DF_ComparisonX.visualize('diagnosis')\n",
    "\n",
    "DF_ComparisonX.writeImages()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the course material, my limited knowledge, and extensive research, I have chosen to compare the results of the following machine learning models: Deep Learning-based Features, Gradient-based Features, Color Histograms, Local Binary Patterns (LBP), and Texture Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_Blanced =  DF_apply_LabelEncoder(df_Blanced, 'diagnosis').encode()\n",
    "df_Blanced=  DF_apply_LabelEncoder(df_Blanced, 'anatomy_sites').encode()\n",
    "df_Blanced =  DF_apply_LabelEncoder(df_Blanced, 'sex').encode()\n",
    "df_Blanced =  DF_apply_LabelEncoder(df_Blanced, 'benign_malignant').encode()\n",
    "df_Blanced['age_approx'] = df_Blanced['age_approx'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_Blanced  = pd.read_csv(Post_scv_file)\n",
    "# df_Blanced.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class plot_Socres:\n",
    "# Assuming you have a model (clf) and data (X, y)\n",
    "    def __init__(self, clf,X,y,num_iterations,filelabel):\n",
    "        self.clf = clf\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_iterations = num_iterations\n",
    "        self.filelabel = filelabel\n",
    "        self.outfutfile = \"output.md\"\n",
    "\n",
    "    def PlotScoer(self):\n",
    "\n",
    "            execution_times = []\n",
    "            accuracy_scores = []\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "\n",
    "            for i in range(self.num_iterations):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Split the data into training and test sets\n",
    "                X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=i)\n",
    "                \n",
    "                # Fit the model and make predictions\n",
    "                self.clf.fit(X_train, y_train)\n",
    "                y_pred = self.clf.predict(X_test)\n",
    "                \n",
    "                # Calculate performance metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                \n",
    "                # Record execution time and performance metrics\n",
    "                execution_times.append(time.time() - start_time)\n",
    "                accuracy_scores.append(accuracy)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "            # Plot the results\n",
    "            x_values = np.arange(1, self.num_iterations+1)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(x_values, execution_times, marker='o')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Execution Time (s)')\n",
    "            plt.title('Model Execution Time')\n",
    "\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.plot(x_values, accuracy_scores, marker='o')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Accuracy Score')\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(x_values, precision_scores, marker='o')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title('Precision Score')\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.plot(x_values, recall_scores, marker='o')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Recall')\n",
    "            plt.title('Recall Score')\n",
    "\n",
    "                      \n",
    "            plt.tight_layout()\n",
    "            fname2 = f\"media//Score_plot_of_{self.filelabel}.png\"\n",
    "            plt.savefig(fname2)\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "             # Add text annotations for the average scores\n",
    "            avg_accuracy_score = np.mean(accuracy_scores)\n",
    "            avg_precision_score = np.mean(precision_scores)\n",
    "            avg_recall_score = np.mean(recall_scores)\n",
    "            avg_f1_score = np.mean(f1_scores)\n",
    "\n",
    "\n",
    "            markup_code =  '\\n'\n",
    "            markup_code += '\\n'\n",
    "            markup_code +=  f'| ![filelabel]({fname2}) |\\n'\n",
    "            markup_code += '\\n'\n",
    "            markup_code +=  f'avg_accuracy_score : {avg_accuracy_score}\\n'\n",
    "            markup_code += '\\n'\n",
    "            markup_code +=  f'avg_precision_score : {avg_precision_score}\\n'\n",
    "            markup_code += '\\n'\n",
    "            markup_code +=  f'avg_recall_score : {avg_recall_score}\\n'\n",
    "            markup_code += '\\n'\n",
    "            markup_code +=  f'avg_f1_score : {avg_f1_score}\\n'\n",
    "            markup_code += '\\n'\n",
    "            \n",
    "\n",
    "            with FileWriter(self.outfutfile ) as writer:\n",
    "                writer.append_to_file(self.filelabel)\n",
    "                writer.append_to_file(markup_code)\n",
    "                writer.append_to_file('\\n')\n",
    "            \n",
    "    def writeImages(self):\n",
    "\n",
    "        markup_code =  '\\n'\n",
    "        markup_code += '\\n'\n",
    "        markup_code +=  f'| ![filelabel]({self.filelabel}) ||\\n'\n",
    "\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "            writer.append_to_file(\"Bfeor and after \")\n",
    "            writer.append_to_file(markup_code)\n",
    "            writer.append_to_file('\\n')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store results for visualization, create an empty array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "# Load the dataset (assuming you have X as image paths and y as corresponding labels)\n",
    "X = np.array([io.imread(os.path.join(Image_File_Path_post_train, path + '.jpg'), as_gray=True) for path in df_Blanced['image_name']])\n",
    "y = np.array(df_Blanced['target'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color Histograms: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# Suppress the warning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Extract color histograms as features\n",
    "X_features = np.array([np.histogram(image.flatten(), bins=256, range=(0, 256))[0] for image in X])\n",
    "\n",
    "# Train the classifier (assuming SVC)\n",
    "svm_model = SVC()\n",
    "\n",
    "plotScore = plot_Socres(svm_model,X_features,y,5,'Color_Histograms')\n",
    "plotScore.PlotScoer()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texture Descriptor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Suppress the UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"skimage.feature.texture\")\n",
    "\n",
    "# Redirect warning messages to null file descriptor\n",
    "with open(os.devnull, \"w\") as devnull:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "# Extract texture descriptors (e.g., LBP) as features\n",
    "X_features = np.array([feature.local_binary_pattern(image, P=8, R=1).flatten() for image in X])\n",
    "\n",
    "\n",
    "plotScore = plot_Socres(svm_model,X_features,y,5,'Texture_Descriptor')\n",
    "plotScore.PlotScoer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-based Features Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract gradient-based features (e.g., HOG) as features\n",
    "X_features = np.array([feature.hog(image) for image in X])\n",
    "\n",
    "# Train the classifier (assuming SVC)\n",
    "svm_model = SVC()\n",
    "\n",
    "plotScore = plot_Socres(svm_model,X_features,y,5,'Gradient-based_Features')\n",
    "plotScore.PlotScoer()\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning-based Features:\n",
    "\n",
    "I was not abel install tensorflow on my computer so I did not evaluate this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from skimage import io\n",
    "\n",
    "# from tensorflow.keras.applications import VGG16\n",
    "# from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "# from tensorflow.keras.models import Model\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# # Preprocess the images (assuming resizing and normalization)\n",
    "# # X_preprocessed = np.array([preprocess_image(image) for image in X])  # Replace 'preprocess_image' with your preprocessing function\n",
    "# X_preprocessed = preprocess_input(X)\n",
    "# # Load pre-trained VGG16 model without the top (fully connected) layers\n",
    "# base_model = VGG16(weights='imagenet', include_top=False)\n",
    "# model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# # Extract deep learning-based features using VGG16\n",
    "# X_features = model.predict(X_preprocessed)\n",
    "\n",
    "# # Flatten the features\n",
    "# X_features = X_features.reshape(X_features.shape[0], -1)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the classifier (assuming SVC)\n",
    "# svm_model = SVC()\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Binary Patterns (LBP) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Suppress the UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"skimage.feature.texture\")\n",
    "\n",
    "#  Extract LBP features as features\n",
    "X_features = np.array([feature.local_binary_pattern(image, P=8, R=1).flatten() for image in X])\n",
    "\n",
    "# Train the classifier (assuming SVC)\n",
    "svm_model = SVC()\n",
    "\n",
    "plotScore = plot_Socres(svm_model,X_features,y,5,'Local_Binary_atterns')\n",
    "plotScore.PlotScoer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Execution_Times)\n",
    "# print(accuracys)\n",
    "# print(precisions)\n",
    "# print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "Write_Readme_file()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
