{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract: This notebook will be used for the capstone project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "import base64\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlity functios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileWriter:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.file = open(self.filename, 'a')\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.file.close()\n",
    "\n",
    "    def append_to_file(self, text):\n",
    "        self.file.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Write_Readme_file():\n",
    "    \n",
    "    # names of the files to be concatenated\n",
    "    file1 = \"README_Org.md\"\n",
    "    file2 = \"output.md\"\n",
    "\n",
    "    # name of the output file\n",
    "    output_file = \"README.md\"\n",
    "\n",
    "    # open the output file in write mode\n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        # open the first file in read mode\n",
    "        with open(file1, \"r\") as infile:\n",
    "            # write its contents to the output file\n",
    "            content = infile.read()\n",
    "            # print(f\"Content of {file1}:\")\n",
    "            # print(content)\n",
    "            outfile.write(content)\n",
    "\n",
    "        # write a newline to separate the contents of the two files\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "        # open the second file in read mode\n",
    "        with open(file2, \"r\") as infile:\n",
    "            # write its contents to the output file\n",
    "            content = infile.read()\n",
    "            # print(f\"Content of {file2}:\")\n",
    "            # print(content)\n",
    "            outfile.write(content)\n",
    "    outfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Visualize_process_dataset:\n",
    "    def __init__(self, Image_fiel_path,csv_file_path,Org_csv_file,Balaced_csv_file, corrupt_dir, output_csv):\n",
    "        \n",
    "        self.df = pd.read_csv(self.Org_csv_file)\n",
    "        self.corrupt_dir = corrupt_dir\n",
    "        self.output_csv = output_csv\n",
    "        self.Org_csv_file = Org_csv_file\n",
    "        self.Image_fiel_path = Image_fiel_path\n",
    "\n",
    "    def Rename_colum(self,Nem_colum_name):\n",
    "        self.df.rename(columns=Nem_colum_name, inplace=True) # rename column name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A balanced dataset for image categorization means that there are roughly equal numbers of images for each class or category in the dataset. You can check the balance of your dataset by counting the number of instances of each class\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the dataset and balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlanceDataste:\n",
    "    def __init__(self, df, output_csv, featuresList):\n",
    "        self.df = df\n",
    "        self.output_csv = output_csv\n",
    "        self.featuresList = featuresList\n",
    "        print(self.featuresList)\n",
    "\n",
    "    def process_Data(self):\n",
    "        try:\n",
    "            # Separate features and target variables\n",
    "             features = self.df.drop(self.featuresList, axis=1)  # Replace with your actual column names\n",
    "             targets = self.df[self.featuresList]  # Replace with your actual column names\n",
    "            \n",
    "\n",
    "        # Perform balancing for each target variable separately\n",
    "             balanced_features = []\n",
    "             balanced_targets = []\n",
    "\n",
    "             for column in targets.columns:\n",
    "                target = targets[column]\n",
    "                \n",
    "                # Perform undersampling\n",
    "                undersampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "                features_undersampled, target_undersampled = undersampler.fit_resample(features, target)\n",
    "                \n",
    "                # Perform oversampling on the undersampled data\n",
    "                oversampler = RandomOverSampler(sampling_strategy='minority')\n",
    "                features_balanced, target_balanced = oversampler.fit_resample(features_undersampled, target_undersampled)\n",
    "                \n",
    "                balanced_features.append(features_balanced)\n",
    "                balanced_targets.append(target_balanced)\n",
    "\n",
    "                # Create a new DataFrame with balanced data\n",
    "                balanced_df = pd.concat([pd.DataFrame(feature) for feature in balanced_features] + [pd.DataFrame(target) for target in balanced_targets], axis=1)\n",
    "\n",
    "                # Write the balanced data to a new CSV file\n",
    "                balanced_df.to_csv(self.output_csv, index=False)\n",
    "                \n",
    "        except IOError:\n",
    "                    print(f\"Error creating image {self.output_csv}\")\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, df, label):\n",
    "        self.df = df\n",
    "        self.scaler = StandardScaler()\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.outfutfile = \"output.md\"\n",
    "        self.label = label\n",
    "        \n",
    "\n",
    "       \n",
    "    def report_and_recommend(self):\n",
    "        \n",
    "        title = (\"\\n Recommendations:\")\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "            writer.append_to_file(title)\n",
    "            for column in self.df.columns:\n",
    "                if self.df[column].isnull().sum() > 0:\n",
    "                        writer.append_to_file(f\" \\n  Column '{column}' has missing values. Consider using dropna() or fillna().\")\n",
    "                            \n",
    "\n",
    "                if self.df[column].dtype == 'object':\n",
    "                        writer.append_to_file(f\" \\n  Column '{column}' is categorical. Consider encoding it using label encoding or one-hot encoding.\")\n",
    "\n",
    "\n",
    "                elif self.df[column].dtype in ['int64', 'float64']:\n",
    "                        writer.append_to_file(f\" \\n  Column '{column}' is numerical. Consider scaling it using standard scaling or min-max scaling.\")\n",
    "        \n",
    "            writer.append_to_file(\"\\n Recommendations - END -:\")        \n",
    "   \n",
    "    def visualize(self, column):\n",
    "               \n",
    "            if self.df[column].dtype in ['int64', 'float64']:\n",
    "\n",
    "                fname = f\"media//Histogram_of_{self.label}_{column}.png\" \n",
    "                ax = sns.histplot(self.df[column])\n",
    "                ax.set_title(f\"Histogram of _{self.label}_{column}\")\n",
    "\n",
    "                plt.xticks(rotation=90) \n",
    "                plt.xlabel(column)\n",
    "                plt.savefig(fname)\n",
    "\n",
    "                with FileWriter(self.outfutfile ) as writer:\n",
    "                    markdown_image_syntax = f\"![Image]({fname})\\n\"\n",
    "                    writer.append_to_file(markdown_image_syntax) \n",
    "                # plt.show()\n",
    "                \n",
    "                # Save the plot as a png image\n",
    "                \n",
    "                # Markdown syntax for displaying an image\n",
    "                #markdown_image_syntax = f\"![Image](media\\{fname})\\n\"\n",
    "                # binary_fc       = open(fname, 'rb').read()  # fc aka file_content\n",
    "                # base64_utf8_str = base64.b64encode(binary_fc).decode('utf-8')\n",
    "\n",
    "                #ext     = filepath.split('.')[-1]\n",
    "                # dataurl = f'data:base64,{base64_utf8_str}'\n",
    "                # markdown_image_syntax = dataurl\n",
    "                \n",
    "            else:\n",
    "\n",
    "                fname = f\"media//Count_plot_of_{self.label}_{column}.png\"\n",
    "                ax = sns.countplot(x=column, data=self.df)\n",
    "                ax.set_title(f\"Count plot of _{self.label}_{column}\")\n",
    "                plt.xticks(rotation=90) \n",
    "                plt.savefig(fname)\n",
    "\n",
    "                with FileWriter(self.outfutfile ) as writer:\n",
    "                    markdown_image_syntax = f\"![Image]({fname})\\n\"\n",
    "                    writer.append_to_file(markdown_image_syntax)\n",
    "                    \n",
    "                # plt.show()\n",
    "                \n",
    "                \n",
    "           \n",
    "            \n",
    "   \n",
    "    def drop_missing(self):\n",
    "        self.df.dropna(inplace=True)\n",
    "\n",
    "    def fill_missing(self, column, value):\n",
    "        self.df[column].fillna(value, inplace=True)\n",
    "\n",
    "    def encode_categorical(self, column):\n",
    "        self.df[column] = self.encoder.fit_transform(self.df[column])\n",
    "\n",
    "    def scale_numerical(self, column):\n",
    "        self.df[column] = self.scaler.fit_transform(self.df[column].values.reshape(-1, 1))\n",
    "\n",
    "    def fill_missing_numeric_With_mean(self, column1, column2):\n",
    "        # Calculate the mean of 'column1' for each category in 'column2'\n",
    "        mean_values = self.df.groupby(column2)[column1].mean()\n",
    "\n",
    "        # Fill the missing values in 'column1' with the mean of 'column1' in the same category in 'column2'\n",
    "        self.df[column1] = self.df.apply(\n",
    "             lambda row: mean_values[row[column2]] if pd.isnull(row[column1]) else row[column1],axis=1)\n",
    "        \n",
    "    def fill_missing_Category_values(self, column1, column2):\n",
    "       \n",
    "        # # Define a lambda function to compute the mode\n",
    "        fill_mode = lambda x: x.fillna(x.mode().iloc[0])\n",
    "\n",
    "        # # Fill missing values in column1 column with the mode of the corresponding group in column2\n",
    "        self.df[column1] = self.df.groupby(column2)[column1].transform(fill_mode)\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class removeCorruptImage:\n",
    "    def __init__(self, Image_fiel_path, df, corrupt_dir, output_csv ):\n",
    "        self.df = df\n",
    "        self.corrupt_dir = corrupt_dir\n",
    "        self.output_csv = output_csv\n",
    "        self.Image_fiel_path = Image_fiel_path\n",
    "        \n",
    "    def is_corrupt(self, file_path):\n",
    "        try:\n",
    "            Image.open(file_path)\n",
    "        except IOError:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def process_images(self): \n",
    "        print('--- Start --')\n",
    "        count = 0\n",
    "        ccount = 0\n",
    "        for index, row in self.df.iterrows():\n",
    "            count = count + 1\n",
    "            file_name = row['image_name'] +'.jpg'\n",
    "            row_image_file = os.path.join(self.Image_fiel_path, file_name)\n",
    "            if self.is_corrupt(row_image_file):\n",
    "                cccount = cccount +1\n",
    "                shutil.move(row_image_file, self.corrupt_dir)  # Move corrupt files to a separate directory\n",
    "                self.df.drop(index, inplace=True)  # Remove the row from the DataFrame\n",
    "                print(f'---curpet-- { row_image_file}')\n",
    "                        \n",
    "            pass\n",
    "        \n",
    "        print('--- End --')\n",
    "        print(count)\n",
    "        print(ccount)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlanceImages:\n",
    "    def __init__(self, Image_fiel_path, df, FinalImage_dir, output_csv,  size=(32, 32) ):\n",
    "        self.df = df\n",
    "        self.FinalImage_dir = FinalImage_dir\n",
    "        self.output_csv = output_csv\n",
    "        self.Image_fiel_path = Image_fiel_path\n",
    "        self.size = size \n",
    "def process_image(self, file_path):\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            img = img.resize(self.size)  # Resize image\n",
    "            # Convert image to numpy array\n",
    "            img_array = np.array(img)\n",
    "\n",
    "            # Normalize to [0,1]\n",
    "            img_normalized = img_array / 255.\n",
    "           # Convert back to image\n",
    "            img = Image.fromarray((img_normalized * 255).astype(np.uint8))\n",
    "            img.save(self.FinalImage_dir)\n",
    "           \n",
    "            \n",
    "        except IOError:\n",
    "            print(f\"Error processing image {file_path}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "def process_images(self): \n",
    "        print('--- Start --')\n",
    "        for index, row in self.df.iterrows():\n",
    "            file_name = row['image_name'] +'.jpg'\n",
    "            row_image_file = os.path.join(self.Image_fiel_path, file_name)\n",
    "            final_Image_Path = os.path.join(self.self.FinalImage_dir, file_name)\n",
    "            if self.process_image(row_image_file):\n",
    "                shutil.move(row_image_file, self.FinalImage_dir)  # Move corrupt files to a separate directory\n",
    "                \n",
    "                        \n",
    "            pass\n",
    "        print('--- Start --')\n",
    "        # self.df.to_csv(self.output_csv,index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_Comparison:\n",
    "    def __init__(self, df1, df2):\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "        self.outfutfile = \"output.md\"\n",
    "\n",
    "       \n",
    "    def report(self):\n",
    "\n",
    "        with open(self.outfutfile , 'w'):\n",
    "             pass\n",
    "\n",
    "        self.print_info(self.df1,\"\\n ## DataFrame info befor process :\")\n",
    "        self.print_Missing_values(self.df1,\"\\n ## Missing values befor & aftre  process :\")\n",
    "             \n",
    "        # print(\"\\nUnique values df1 - df2:\")\n",
    "        title = \"\\n ## Unique values befor & aftre  process :\"\n",
    "        self.print_unique_values(title)\n",
    "\n",
    "        self.print_value_count(\"\\n ## Value counts befor & aftre  process :\")\n",
    "        title = \"\\n ## Descriptive statistics befor and after the process:\"\n",
    "\n",
    "\n",
    "\n",
    "        # Getting the description of both DataFrames\n",
    "        desc_df1 = self.df1.describe()\n",
    "        desc_df2 = self.df2.describe()\n",
    "\n",
    "        # Combine the two descriptions into one DataFrame\n",
    "        combined_desc = pd.concat([desc_df1, desc_df2], axis=1)\n",
    "\n",
    "        markdown_str = combined_desc.to_markdown()\n",
    "\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "            writer.append_to_file(title)\n",
    "            writer.append_to_file(markdown_str)\n",
    "            writer.append_to_file('\\n')\n",
    "\n",
    "      \n",
    "\n",
    "        \n",
    "    def print_info(self,df,title):\n",
    "\n",
    "        \n",
    "        # Capture df.info() output in a string\n",
    "        buf = io.StringIO()\n",
    "        df.info(buf=buf)\n",
    "        info_str = buf.getvalue()\n",
    "\n",
    "        # Create a summary DataFrame\n",
    "        info_list = info_str.split('\\n')[5:-3]  # Remove first and last two lines\n",
    "        info_data = [line.split() for line in info_list]  # Split each line into list of words\n",
    "        # Create dataframe from info_data\n",
    "        info_df = pd.DataFrame(info_data, columns=['index', 'Name', 'Count', 'Non-Null','Dtype'])\n",
    "\n",
    "        # Print the summary DataFrame in markdown format\n",
    "        # print(info_df.to_markdown(index=False))\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        markdown_str = info_df.to_markdown()\n",
    "\n",
    "        # Write the markdown string to a text file\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "            writer.append_to_file(title)\n",
    "            writer.append_to_file(markdown_str)\n",
    "            writer.append_to_file('\\n')\n",
    "    \n",
    "    \n",
    "    def print_Missing_values(self,df,title):\n",
    "\n",
    "        # Get the count of null values in each column\n",
    "        null_counts1 = self.df1.isnull().sum()\n",
    "        null_counts2 = self.df2.isnull().sum()\n",
    "        null_counts = pd.DataFrame({'Befor':null_counts1,'After':null_counts2})\n",
    "        \n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "             writer.append_to_file(title)\n",
    "             writer.append_to_file(null_counts.to_markdown())\n",
    "             writer.append_to_file('\\n')\n",
    "\n",
    "    def print_unique_values(self,title):\n",
    "\n",
    "        unique_counts1 = self.df1.nunique()\n",
    "        unique_counts2 = self.df2.nunique()\n",
    "        unique_counts = pd.DataFrame({'Befor':unique_counts1,'After':unique_counts2})\n",
    "\n",
    "        #print(unique_counts)    \n",
    "        # Write the markdown string to a text file\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "             writer.append_to_file(title)\n",
    "             writer.append_to_file(unique_counts.to_markdown())\n",
    "             writer.append_to_file('\\n')\n",
    "\n",
    "    def print_value_count(self,title):\n",
    "\n",
    "        # Create an empty DataFrame to store the results\n",
    "        result = pd.DataFrame()\n",
    "        \n",
    "        # Iterate over each column\n",
    "        for column in self.df1.columns:\n",
    "\n",
    "            if self.df1[column].nunique() > 8:\n",
    "                continue\n",
    "\n",
    "            df1_counts = self.df1[column].value_counts()\n",
    "            df2_counts = self.df2[column].value_counts()\n",
    "\n",
    "            # Combine the two Series into a DataFrame\n",
    "            temp_df = pd.concat([df1_counts, df2_counts], axis=1, keys=['DF1', 'DF2'])\n",
    "\n",
    "            # Add the column name to the DataFrame\n",
    "            temp_df['Column'] = column\n",
    "\n",
    "            # Append the temporary DataFrame to the result DataFrame\n",
    "            \n",
    "            result = pd.concat([result, temp_df])\n",
    "            \n",
    "\n",
    "        # Reset the index of the result DataFrame\n",
    "        result.reset_index(inplace=True)\n",
    "\n",
    "        # Rename the columns for clarity\n",
    "        result.columns = ['Value', 'Count Befor', 'Count After', 'Column']\n",
    "\n",
    "        # Rearrange the columns\n",
    "        result = result[['Column', 'Value', 'Count Befor', 'Count After']]\n",
    "\n",
    "        with FileWriter(self.outfutfile ) as writer:\n",
    "             writer.append_to_file(title)\n",
    "             writer.append_to_file(result.to_markdown())\n",
    "             writer.append_to_file('\\n')\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DF_apply_LabelEncoder:\n",
    "    def __init__(self, df, colName):\n",
    "        self.df = df\n",
    "        self.colName = colName\n",
    "        self.outfutfile = \"output.md\"\n",
    "\n",
    "    def encode(self):\n",
    "    # Label Encoding\n",
    "        label_encoder = LabelEncoder()\n",
    "        Label = self.colName + \"_\" + \"encoded\"\n",
    "        self.df[Label] = label_encoder.fit_transform(self.df[self.colName])\n",
    "    # Return the transformed DataFrame\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_apply_StandardScaler:\n",
    "    def __init__(self, df, colName):\n",
    "        self.df = df\n",
    "        self.colName = colName\n",
    "        self.outfutfile = \"output.md\"\n",
    "\n",
    "    def encode(self):\n",
    "    # Label Encoding\n",
    "        scaler = StandardScaler()\n",
    "        Label = self.colName + \"_\" + \"encoded\"\n",
    "        self.df[Label] = scaler.fit_transform(self.df[self.colName])\n",
    "    # Return the transformed DataFrame\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Final_CapStone_Project\n",
      "c:\\\n",
      "c:\\Train\n",
      "c:\\Final_CapStone_Project\\Data_path_Name\\jpeg\\Train\n",
      "--- Start --\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'cccount' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m# # # Remove currupt Image files\u001b[39;00m\n\u001b[0;32m     45\u001b[0m processor \u001b[39m=\u001b[39m removeCorruptImage(Image_File_Path_pre_train , df_org , \u001b[39m'\u001b[39m\u001b[39mcorrupt_files\u001b[39m\u001b[39m'\u001b[39m, Post_scv_file)\n\u001b[1;32m---> 46\u001b[0m processor\u001b[39m.\u001b[39;49mprocess_images()\n\u001b[0;32m     48\u001b[0m df_new \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     50\u001b[0m \u001b[39m# featuresList = ['benign_malignant']\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m# Blancer = BlanceDataste(df_new,Blanced_csv_file,featuresList)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m# Blancer.process_Data()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[39m# add semple Images befor resizing to README\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[74], line 30\u001b[0m, in \u001b[0;36mremoveCorruptImage.process_images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m---curpet-- \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m \u001b[39mrow_image_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m ([count,cccount])\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m--- Start --\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'cccount' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Define row CSV file name and path\n",
    "\n",
    "# Current directory\n",
    "current_dir = os.getcwd()\n",
    "# print('-------------------')\n",
    "print(current_dir)\n",
    "\n",
    "# Parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "print(parent_dir)\n",
    "\n",
    "Data_path_Name= 'melanoma-classification_data/'\n",
    "Train_File_name_pre = 'train_pre.csv'\n",
    "\n",
    "\n",
    "# Test_File_name_pre = 'test_pre.csv'\n",
    "# Test_File_name_post = 'test_post.csv'\n",
    "# C:\\AL-ML\\ML-Mod-24\\CapStone_project\\melanoma-classification_data\\jpeg\n",
    "\n",
    "Image_File_Path_pre_train = os.path.join(parent_dir, '\\\\Train')\n",
    "Image_File_Path_post_train = os.path.join(current_dir, 'Data_path_Name\\jpeg\\Train')\n",
    "print(Image_File_Path_pre_train)\n",
    "print(Image_File_Path_post_train)\n",
    "# Image_File_Path_pre_test = os.path.join(current_dir, 'Images\\Pre_Images\\Test')\n",
    "# Image_File_Path_post_test = os.path.join(current_dir, 'Images\\\\Post_Images\\Test')\n",
    "\n",
    "Org_csv_file = os.path.join(Data_path_Name, Train_File_name_pre)\n",
    "\n",
    "\n",
    "# # test_csv_file = os.path.join(Csv_path_Name, Test_File_name_pre)\n",
    "\n",
    "Train_cleand_csv_File = 'Blanced.csv'\n",
    "Blanced_csv_file = os.path.join(Data_path_Name, Train_cleand_csv_File)\n",
    "Test_cleand_csv_File = ''\n",
    "\n",
    "# print(Org_csv_file)\n",
    "# print(current_dir)\n",
    "df_org  = pd.read_csv(Org_csv_file)\n",
    "\n",
    "df_org.rename(columns={\"anatom_site_general_challenge\": \"anatomy_sites\"}, inplace=True) # rename column name\n",
    "\n",
    "\n",
    "\n",
    "# # # Remove currupt Image files\n",
    "processor = removeCorruptImage(Image_File_Path_pre_train , df_org , 'corrupt_files', Post_scv_file)\n",
    "processor.process_images()\n",
    "\n",
    "df_new = processor.df.copy()\n",
    "\n",
    "# featuresList = ['benign_malignant']\n",
    "# Blancer = BlanceDataste(df_new,Blanced_csv_file,featuresList)\n",
    "# Blancer.process_Data()\n",
    "\n",
    "\n",
    "\n",
    "# df_Blanced  = pd.read_csv(Blanced_csv_file)\n",
    "# DF_ComparisonX = DF_Comparison(df_new, df_Blanced)\n",
    "# DF_ComparisonX.report()\n",
    "\n",
    "# cleaner_1 = DataCleaner(df_new, 'Befro_Blancing')\n",
    "\n",
    "# cleaner_1.visualize('diagnosis')\n",
    "# cleaner_1.visualize('sex')\n",
    "# cleaner_1.visualize('age_approx')\n",
    "# cleaner_1.visualize('anatomy_sites')\n",
    "# cleaner_1.visualize('target')\n",
    "# cleaner_1.visualize('benign_malignant')\n",
    "\n",
    "# cleaner_2 = DataCleaner(df_Blanced, 'After_Blancing')\n",
    "\n",
    "# cleaner_2.fill_missing_numeric_With_mean('age_approx', 'diagnosis')\n",
    "# cleaner_2.fill_missing_Category_values('anatomy_sites', 'diagnosis')\n",
    "# cleaner_2.fill_missing_Category_values('sex', 'diagnosis')\n",
    "\n",
    "\n",
    "# cleaner_2.visualize('diagnosis')\n",
    "# cleaner_2.visualize('sex')\n",
    "# cleaner_2.visualize('age_approx')\n",
    "# cleaner_2.visualize('anatomy_sites')\n",
    "# cleaner_2.visualize('target')\n",
    "# cleaner_2.visualize('benign_malignant')\n",
    "\n",
    "# cleaner_2.report_and_recommend()\n",
    "\n",
    "# Write_Readme_file()\n",
    "\n",
    "# Blanced_df =  DF_apply_LabelEncoder(cleaner_2.df, 'diagnosis').encode()\n",
    "# Blanced_df =  DF_apply_LabelEncoder(Blanced_df, 'anatomy_sites').encode()\n",
    "# Blanced_df =  DF_apply_LabelEncoder(Blanced_df, 'sex').encode()\n",
    "# Blanced_df =  DF_apply_LabelEncoder(Blanced_df, 'benign_malignant').encode()\n",
    "# Blanced_df['age_approx'] = Blanced_df['age_approx'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(df_new.head(5))\n",
    "# print(df_org.describe())\n",
    "# print(df_org.info())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add semple Images befor resizing to README\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
